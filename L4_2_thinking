Thinking1：奇异值分解SVD的原理是怎样的，都有哪些应用场景
答：SVD的原理是基于EVD的扩展。EVD是通过对某一方阵求其特征值和特征向量，将其展开成3个矩阵相乘的形式。
    而SVD是对一般矩阵进行拆解，如矩阵A(m,n)，构造A.dot(A.T)和A.T.dot(A)，得到两个方阵，进而进行EVD，得到左奇异矩阵、右奇异矩阵和特征值，完成分解。
    SVD可以应用于推荐系统中，对user对item的评分进行预测，同时还可以实现降维。

Thinking2：funkSVD, BiasSVD，SVD++算法之间的区别是怎样的
答：三种SVD算法都应用于推荐系统中的评分预测问题，但再考虑的特征方面存在差异：
    funkSVD就是直接对用户对商品的评分矩阵进行分解，采取SGD的优化方式，并在损失函数中增加了惩罚项防止过拟合；
    BiasSVD则在funkSVD的基础上考虑了用户的偏好和产品的偏好（类似于Baseline模型），并且也加入到了惩罚项之中；
    SVD++在BiasSVD的基础上又作了扩展，考虑了用户的隐式反馈，以用户i对商品j的隐式反馈修正值C_ij描述，同时也进一步考虑用户间隐式反馈的个性，消除相同隐式行为的影响（除以|I(i)|，|I(i)|为用户隐式行为集合）。

Thinking3：矩阵分解算法在推荐系统中有哪些应用场景，存在哪些不足
答：矩阵分解再推荐系统中的主要应用就是评分预测，同时还可以通过“降维”进一步提高推荐系统的效率。
    其不足之处主要有：
    1）仅考虑了用户和商品两个维度的特征，无法考虑更多特征的影响；
    2）矩阵分解的前提条件是要求矩阵是稠密的，但实际情况通常得到的是稀疏矩阵，使用近似方法填充会对预测结果的准确性带来较大影响，因此在冷启动情况下不太适用。

Thinking4：假设一个小说网站，有N部小说，每部小说都有摘要描述。如何针对该网站制定基于内容的推荐系统，即用户看了某部小说后，推荐其他相关的小说。原理和步骤是怎样的
答：思路1：
    1）对小说的摘要描述进行特征提取，设置n_gram，生成TF-IDF矩阵；
    2）计算两两小说摘要间的余弦相似度；
    3）对于指定小说，选择相似度最大的K个小说进行推荐。
   思路2（非基于内容的推荐）：
    1）寻找看过这部小说的所有用户；
    2）提取用户的观看历史行为，按用户行为生成向量
    3）计算用户观看行为的相似性，
    4）找到与该用户行为相似度最高的K个用户，统计这K个用户观看完这个小说后共同所看的小说，作为推荐。

Thinking5：Word2Vec的应用场景有哪些
答：Word Embedding就是将Word嵌入到一个数学空间里，Word2vec，就是词嵌入的一种
可以将sentence中的word转换为固定大小的向量表达（Vector Respresentations），
其中意义相近的词将被映射到向量空间中相近的位置。
将待解决的问题转换成为单词word和文章doc的对应关系
大V推荐中，大V => 单词，将每一个用户关注大V的顺序 => 文章
商品推荐中，商品 => 单词，用户对商品的行为顺序 => 文章

